{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cute little demo showing the simplest usage of minGPT. Configured to run fine on Macbook Air in like a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.utils import set_seed\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# class SortDataset(Dataset):\n",
    "#     \"\"\" \n",
    "#     Dataset for the Sort problem. E.g. for problem length 6:\n",
    "#     Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "#     Which will feed into the transformer concatenated as:\n",
    "#     input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "#     output: I I I I I 0 0 0 1 1 2\n",
    "#     where I is \"ignore\", as the transformer is reading the input sequence\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, split, length=6, num_digits=3):\n",
    "#         assert split in {'train', 'test'}\n",
    "#         self.split = split\n",
    "#         self.length = length\n",
    "#         self.num_digits = num_digits\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return 10000 # ...\n",
    "    \n",
    "#     def get_vocab_size(self):\n",
    "#         return self.num_digits\n",
    "    \n",
    "#     def get_block_size(self):\n",
    "#         # the length of the sequence that will feed into transformer, \n",
    "#         # containing concatenated input and the output, but -1 because\n",
    "#         # the transformer starts making predictions at the last input element\n",
    "#         return self.length * 2 - 1\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         # use rejection sampling to generate an input example from the desired split\n",
    "#         while True:\n",
    "#             # generate some random integers\n",
    "#             inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "#             # half of the time let's try to boost the number of examples that \n",
    "#             # have a large number of repeats, as this is what the model seems to struggle\n",
    "#             # with later in training, and they are kind of rate\n",
    "#             if torch.rand(1).item() < 0.5:\n",
    "#                 if inp.unique().nelement() > self.length // 2:\n",
    "#                     # too many unqiue digits, re-sample\n",
    "#                     continue\n",
    "#             # figure out if this generated example is train or test based on its hash\n",
    "#             h = hash(pickle.dumps(inp.tolist()))\n",
    "#             inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "#             if inp_split == self.split:\n",
    "#                 break # ok\n",
    "        \n",
    "#         # solve the task: i.e. sort\n",
    "#         sol = torch.sort(inp)[0]\n",
    "\n",
    "#         # concatenate the problem specification and the solution\n",
    "#         cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "#         # the inputs to the transformer will be the offset sequence\n",
    "#         x = cat[:-1].clone()\n",
    "#         y = cat[1:].clone()\n",
    "#         # we only want to predict at output locations, mask out the loss at the input locations\n",
    "#         y[:self.length-1] = -1\n",
    "#         return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 2, 5, 3, 6, 0, 0, 0, 3, 7, 2, 0, 0, 1, 8, 6, 0, 0, 3, 1, 0, 0, 0, 0, 3, 3, 2, 3, 2]\n",
      "[4, 9, 5, 3, 7, 8, 0, 0, 3, 9, 6, 0, 0, 3, 4, 6, 5, 0, 1, 4, 8, 5, 0, 0, 1, 8, 7, 1, 1, 0]\n",
      "[6, 4, 4, 3, 1, 4, 0, 0, 2, 5, 7, 6, 0, 0, 6, 4, 4, 0, 1, 9, 3, 2, 0, 0, 2, 0, 2, 2, 1, 6]\n",
      "[3, 5, 9, 1, 1, 9, 0, 0, 3, 2, 3, 1, 0, 0, 3, 5, 9, 0, 0, 3, 5, 9, 0, 0, 0, 4, 2, 7, 2, 1]\n",
      "[3, 5, 0, 1, 4, 8, 0, 0, 2, 8, 0, 0, 0, 1, 4, 0, 0, 0, 0, 3, 5, 0, 0, 0, 0, 5, 1, 8, 0, 0]\n",
      "[0, 0, 2, 9, 3, 7, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 6, 0, 0, 0, 1, 8, 0, 0, 0, 0, 1, 8, 7, 4]\n",
      "[8, 4, 8, 2, 3, 7, 0, 0, 5, 9, 3, 6, 0, 2, 5, 4, 4, 0, 1, 6, 9, 6, 0, 0, 2, 0, 0, 9, 7, 6]\n",
      "[9, 9, 9, 7, 0, 6, 0, 0, 5, 9, 9, 4, 0, 0, 0, 0, 0, 0, 6, 9, 9, 3, 0, 0, 7, 0, 5, 2, 9, 4]\n",
      "[7, 4, 5, 1, 3, 6, 0, 0, 4, 4, 7, 0, 0, 2, 2, 3, 5, 0, 0, 7, 4, 5, 0, 0, 1, 0, 1, 3, 2, 0]\n",
      "[1, 0, 1, 0, 4, 2, 0, 0, 0, 2, 0, 2, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "def pad_with_zeros_str_mult(s, length):\n",
    "    return (2 * length - len(s)) * '0' + s if (2 * length - len(s)) >0 else s\n",
    "\n",
    "def random_add_instance(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "    val_b = int(''.join(str(d) for d in b))\n",
    "    val_c = val_a + val_b\n",
    "    str_c = str(val_c)\n",
    "    str_c = (length + 1 - len(str_c)) * '0' + str_c\n",
    "    return a + b + [int(d) for d in str_c]\n",
    "\n",
    "def random_multiply_instance(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "    val_b = int(''.join(str(d) for d in b))\n",
    "    val_c = val_a * val_b\n",
    "    str_c = str(val_c)\n",
    "    str_c = (2 * length - len(str_c)) * '0' + str_c if (2 * length - len(str_c)) >0 else str_c\n",
    "    return a + b + [int(d) for d in str_c]\n",
    "\n",
    "def random_multiply_long_instance(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "    # a = [1, 2 , 3]\n",
    "    # b = [4, 5, 6]\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "    val_b = int(''.join(str(d) for d in b))\n",
    "    elems = []\n",
    "    ten_pow = 1\n",
    "    for d in b[::-1]:\n",
    "        e_val = val_a * d * ten_pow\n",
    "        e_str = pad_with_zeros_str_mult(str(e_val), length)\n",
    "        elems.append(e_str)\n",
    "        ten_pow *= 10\n",
    "\n",
    "    val_c = val_a * val_b\n",
    "    str_c = str(val_c)\n",
    "    str_c = (2 * length - len(str_c)) * '0' + str_c if (2 * length - len(str_c)) >0 else str_c\n",
    "    return a + b + [int(d) for d in \"\".join(elems)] + [int(d) for d in str_c]\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "#     # print (random_add_instance(3))\n",
    "    print(random_multiply_long_instance(3))\n",
    "#     print(random_multiply_instance(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AddDataset(Dataset):\n",
    "#     \"\"\" \n",
    "#     Dataset for the Add problem. E.g. for problem length 3:\n",
    "#     12 + 333 = 345\n",
    "#     Input: 0 1 2 3 3 3 -> Output: 0 3 4 5\n",
    "#     Which will feed into the transformer concatenated as:\n",
    "#     input:  0 1 2 3 3 3 0 3 4\n",
    "#     output: I I I I I 0 3 4 5\n",
    "#     where I is \"ignore\", as the transformer is reading the input sequence\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, split, length=3):\n",
    "#         assert split in {'train', 'test'}\n",
    "#         self.split = split\n",
    "#         self.length = length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return 10000 # ...\n",
    "    \n",
    "#     def get_vocab_size(self):\n",
    "#         return 10\n",
    "    \n",
    "#     def get_block_size(self):\n",
    "#         # the length of the sequence that will feed into transformer, \n",
    "#         # containing concatenated input and the output, but -1 because\n",
    "#         # the transformer starts making predictions at the last input element\n",
    "#         return 3 * self.length + 1 - 1\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         while True:\n",
    "#             rai = random_add_instance(self.length)\n",
    "#             h = hash(str(rai[:2*self.length]))\n",
    "            \n",
    "#             inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "#             if inp_split == self.split:\n",
    "#                 break # ok\n",
    "        \n",
    "#         x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "#         y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "#         # we only want to predict at output locations, mask out the loss at the input locations\n",
    "#         y[:2*self.length-1] = -1\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Multiply problem. E.g. for problem length 3:\n",
    "    12 * 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 0 3 9 9 6\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 0 3 9 9\n",
    "    output: I I I I I 0 0 3 9 9 6\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 10\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return 4 * self.length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = random_multiply_instance(self.length)\n",
    "            h = hash(str(rai[:2*self.length]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:2*self.length-1] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyLongDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Multiply problem. E.g. for problem length 3:\n",
    "    12 * 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 0 3 9 9 6\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 0 3 9 9\n",
    "    output: I I I I I 0 0 3 9 9 6\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 10\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return 2*self.length + self.length*2*self.length + 2*self.length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = random_multiply_long_instance(self.length)\n",
    "            h = hash(str(rai[:2*self.length]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:2*self.length + self.length*2*self.length - 1] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print an example instance of the dataset\n",
    "# train_dataset = AddDataset('train')\n",
    "# test_dataset = AddDataset('test')\n",
    "# x, y = train_dataset[0]\n",
    "\n",
    "# print (x)\n",
    "# for a, b in zip(x,y):\n",
    "#     print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 9, 3, 4, 8, 0, 1, 4, 0, 6, 4]) 0\n",
      "2 -1\n",
      "9 -1\n",
      "3 -1\n",
      "4 -1\n",
      "8 -1\n",
      "0 1\n",
      "1 4\n",
      "4 0\n",
      "0 6\n",
      "6 4\n",
      "4 0\n"
     ]
    }
   ],
   "source": [
    "train_dataset_m = MultiplyDataset('train')\n",
    "test_dataset_m = MultiplyDataset('test')\n",
    "x_m, y_m = train_dataset_m[0]\n",
    "\n",
    "print (x_m, y_m[-1].item())\n",
    "for a_m, b_m in zip(x_m,y_m):\n",
    "    print(int(a_m),int(b_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 5, 6, 2, 9, 0, 0, 2, 1, 1, 5, 0, 0, 4, 7, 0, 0, 1, 4, 1, 0, 0, 0,\n",
      "        1, 4, 7, 8, 1]) 5\n",
      "2 -1\n",
      "3 -1\n",
      "5 -1\n",
      "6 -1\n",
      "2 -1\n",
      "9 -1\n",
      "0 -1\n",
      "0 -1\n",
      "2 -1\n",
      "1 -1\n",
      "1 -1\n",
      "5 -1\n",
      "0 -1\n",
      "0 -1\n",
      "4 -1\n",
      "7 -1\n",
      "0 -1\n",
      "0 -1\n",
      "1 -1\n",
      "4 -1\n",
      "1 -1\n",
      "0 -1\n",
      "0 -1\n",
      "0 1\n",
      "1 4\n",
      "4 7\n",
      "7 8\n",
      "8 1\n",
      "1 5\n"
     ]
    }
   ],
   "source": [
    "train_dataset_ml = MultiplyLongDataset('train')\n",
    "test_dataset_ml = MultiplyLongDataset('test')\n",
    "x_ml, y_ml = train_dataset_ml[0]\n",
    "\n",
    "print (x_ml, y_ml[-1].item())\n",
    "for a_ml, b_ml in zip(x_ml,y_ml):\n",
    "    print(int(a_ml),int(b_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "\n",
    "\n",
    "# model_config = GPT.get_default_config()\n",
    "# model_config.model_type = 'gpt-micro'\n",
    "# model_config.model_type = 'gpt-nano'\n",
    "\n",
    "# model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "# model_config.block_size = train_dataset.get_block_size()\n",
    "# model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "source": [
    "model_config_m = GPT.get_default_config()\n",
    "model_config_m.model_type = 'gpt-micro'\n",
    "# model_config_m.model_type = 'gpt-nano'\n",
    "\n",
    "model_config_m.vocab_size = train_dataset_m.get_vocab_size()\n",
    "model_config_m.block_size = train_dataset_m.get_block_size()\n",
    "model_m = GPT(model_config_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "source": [
    "model_config_ml = GPT.get_default_config()\n",
    "model_config_ml.model_type = 'gpt-micro'\n",
    "# model_config_m.model_type = 'gpt-nano'\n",
    "\n",
    "model_config_ml.vocab_size = train_dataset_ml.get_vocab_size()\n",
    "model_config_ml.block_size = train_dataset_ml.get_block_size()\n",
    "model_ml = GPT(model_config_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (model_config.n_head, model_config.n_layer, model_config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "\n",
    "# train_config = Trainer.get_default_config()\n",
    "# train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "# train_config.max_iters = 5000\n",
    "# train_config.num_workers = 0\n",
    "# trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "train_config_m = Trainer.get_default_config()\n",
    "train_config_m.learning_rate = 5e-3 # the model we're using is so small that we can go a bit faster\n",
    "train_config_m.max_iters = 10000\n",
    "train_config_m.num_workers = 0\n",
    "\n",
    "# train_config_m.learning_rate = 1e-3  # Lower learning rate for stability\n",
    "# train_config_m.max_iters = 30000     # Reduce if convergence is fast\n",
    "# train_config_m.num_workers = 4       # Enable parallel data loading\n",
    "# train_config_m.batch_size = 64       # Example batch size\n",
    "# train_config_m.lr_scheduler = \"cosine\"  # Optionally use a learning rate scheduler\n",
    "\n",
    "trainer_m = Trainer(train_config_m, model_m, train_dataset_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "train_config_ml = Trainer.get_default_config()\n",
    "train_config_ml.learning_rate = 5e-3 # the model we're using is so small that we can go a bit faster\n",
    "train_config_ml.max_iters = 10000\n",
    "train_config_ml.num_workers = 0\n",
    "\n",
    "# train_config_m.learning_rate = 1e-3  # Lower learning rate for stability\n",
    "# train_config_m.max_iters = 30000     # Reduce if convergence is fast\n",
    "# train_config_m.num_workers = 4       # Enable parallel data loading\n",
    "# train_config_m.batch_size = 64       # Example batch size\n",
    "# train_config_m.lr_scheduler = \"cosine\"  # Optionally use a learning rate scheduler\n",
    "\n",
    "trainer_ml = Trainer(train_config_ml, model_ml, train_dataset_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "# trainer.set_callback('on_batch_end', batch_end_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.run() # addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 2.33108\n",
      "iter_dt 12.69ms; iter 100: train loss 2.15056\n",
      "iter_dt 11.96ms; iter 200: train loss 1.94409\n",
      "iter_dt 11.96ms; iter 300: train loss 1.85359\n",
      "iter_dt 12.02ms; iter 400: train loss 1.91285\n",
      "iter_dt 12.05ms; iter 500: train loss 1.82069\n",
      "iter_dt 11.68ms; iter 600: train loss 1.83571\n",
      "iter_dt 12.11ms; iter 700: train loss 1.83202\n",
      "iter_dt 12.00ms; iter 800: train loss 1.77521\n",
      "iter_dt 11.57ms; iter 900: train loss 1.79282\n",
      "iter_dt 11.97ms; iter 1000: train loss 1.78317\n",
      "iter_dt 10.18ms; iter 1100: train loss 1.73567\n",
      "iter_dt 11.28ms; iter 1200: train loss 1.86182\n",
      "iter_dt 12.03ms; iter 1300: train loss 1.77553\n",
      "iter_dt 12.01ms; iter 1400: train loss 1.80835\n",
      "iter_dt 12.00ms; iter 1500: train loss 1.82002\n",
      "iter_dt 12.07ms; iter 1600: train loss 1.73743\n",
      "iter_dt 12.00ms; iter 1700: train loss 1.75644\n",
      "iter_dt 11.97ms; iter 1800: train loss 1.75930\n",
      "iter_dt 11.99ms; iter 1900: train loss 1.72904\n",
      "iter_dt 11.94ms; iter 2000: train loss 1.67961\n",
      "iter_dt 12.03ms; iter 2100: train loss 1.71690\n",
      "iter_dt 10.39ms; iter 2200: train loss 1.68299\n",
      "iter_dt 12.56ms; iter 2300: train loss 1.68393\n",
      "iter_dt 12.66ms; iter 2400: train loss 1.63256\n",
      "iter_dt 12.04ms; iter 2500: train loss 1.63759\n",
      "iter_dt 12.16ms; iter 2600: train loss 1.70946\n",
      "iter_dt 12.07ms; iter 2700: train loss 1.70984\n",
      "iter_dt 12.02ms; iter 2800: train loss 1.64744\n",
      "iter_dt 11.98ms; iter 2900: train loss 1.64981\n",
      "iter_dt 11.98ms; iter 3000: train loss 1.66776\n",
      "iter_dt 12.01ms; iter 3100: train loss 1.61945\n",
      "iter_dt 11.63ms; iter 3200: train loss 1.65900\n",
      "iter_dt 13.27ms; iter 3300: train loss 1.65066\n",
      "iter_dt 12.01ms; iter 3400: train loss 1.65197\n",
      "iter_dt 11.49ms; iter 3500: train loss 1.63281\n",
      "iter_dt 11.04ms; iter 3600: train loss 1.61123\n",
      "iter_dt 11.56ms; iter 3700: train loss 1.58563\n",
      "iter_dt 12.01ms; iter 3800: train loss 1.62357\n",
      "iter_dt 11.94ms; iter 3900: train loss 1.62079\n",
      "iter_dt 11.99ms; iter 4000: train loss 1.59079\n",
      "iter_dt 10.04ms; iter 4100: train loss 1.59319\n",
      "iter_dt 11.95ms; iter 4200: train loss 1.54009\n",
      "iter_dt 11.97ms; iter 4300: train loss 1.58835\n",
      "iter_dt 11.10ms; iter 4400: train loss 1.61499\n",
      "iter_dt 11.98ms; iter 4500: train loss 1.62025\n",
      "iter_dt 11.97ms; iter 4600: train loss 1.55588\n",
      "iter_dt 12.00ms; iter 4700: train loss 1.50951\n",
      "iter_dt 11.93ms; iter 4800: train loss 1.56947\n",
      "iter_dt 12.24ms; iter 4900: train loss 1.47502\n",
      "iter_dt 11.01ms; iter 5000: train loss 1.51818\n",
      "iter_dt 10.95ms; iter 5100: train loss 1.49173\n",
      "iter_dt 12.04ms; iter 5200: train loss 1.48499\n",
      "iter_dt 12.11ms; iter 5300: train loss 1.54118\n",
      "iter_dt 11.66ms; iter 5400: train loss 1.52950\n",
      "iter_dt 12.00ms; iter 5500: train loss 1.49938\n",
      "iter_dt 12.06ms; iter 5600: train loss 1.47945\n",
      "iter_dt 11.98ms; iter 5700: train loss 1.48524\n",
      "iter_dt 11.98ms; iter 5800: train loss 1.55416\n",
      "iter_dt 11.94ms; iter 5900: train loss 1.50996\n",
      "iter_dt 11.95ms; iter 6000: train loss 1.49450\n",
      "iter_dt 12.03ms; iter 6100: train loss 1.45214\n",
      "iter_dt 12.03ms; iter 6200: train loss 1.46458\n",
      "iter_dt 11.94ms; iter 6300: train loss 1.48260\n",
      "iter_dt 11.96ms; iter 6400: train loss 1.45378\n",
      "iter_dt 13.49ms; iter 6500: train loss 1.42221\n",
      "iter_dt 11.99ms; iter 6600: train loss 1.45425\n",
      "iter_dt 12.01ms; iter 6700: train loss 1.40357\n",
      "iter_dt 12.07ms; iter 6800: train loss 1.44938\n",
      "iter_dt 9.96ms; iter 6900: train loss 1.45765\n",
      "iter_dt 12.09ms; iter 7000: train loss 1.43258\n",
      "iter_dt 9.74ms; iter 7100: train loss 1.37186\n",
      "iter_dt 12.01ms; iter 7200: train loss 1.38368\n",
      "iter_dt 12.01ms; iter 7300: train loss 1.33209\n",
      "iter_dt 12.04ms; iter 7400: train loss 1.40019\n",
      "iter_dt 11.52ms; iter 7500: train loss 1.37319\n",
      "iter_dt 11.65ms; iter 7600: train loss 1.40544\n",
      "iter_dt 12.00ms; iter 7700: train loss 1.38646\n",
      "iter_dt 11.98ms; iter 7800: train loss 1.38539\n",
      "iter_dt 12.04ms; iter 7900: train loss 1.40844\n",
      "iter_dt 11.99ms; iter 8000: train loss 1.27874\n",
      "iter_dt 12.01ms; iter 8100: train loss 1.30563\n",
      "iter_dt 12.00ms; iter 8200: train loss 1.32617\n",
      "iter_dt 12.01ms; iter 8300: train loss 1.37015\n",
      "iter_dt 12.01ms; iter 8400: train loss 1.35194\n",
      "iter_dt 11.57ms; iter 8500: train loss 1.29488\n",
      "iter_dt 11.94ms; iter 8600: train loss 1.32427\n",
      "iter_dt 12.01ms; iter 8700: train loss 1.34765\n",
      "iter_dt 11.76ms; iter 8800: train loss 1.31818\n",
      "iter_dt 12.01ms; iter 8900: train loss 1.31404\n",
      "iter_dt 11.98ms; iter 9000: train loss 1.32194\n",
      "iter_dt 11.97ms; iter 9100: train loss 1.32473\n",
      "iter_dt 12.04ms; iter 9200: train loss 1.33482\n",
      "iter_dt 12.02ms; iter 9300: train loss 1.29452\n",
      "iter_dt 11.95ms; iter 9400: train loss 1.32049\n",
      "iter_dt 11.03ms; iter 9500: train loss 1.31108\n",
      "iter_dt 12.01ms; iter 9600: train loss 1.29719\n",
      "iter_dt 11.99ms; iter 9700: train loss 1.31111\n",
      "iter_dt 12.00ms; iter 9800: train loss 1.24017\n",
      "iter_dt 11.91ms; iter 9900: train loss 1.23605\n"
     ]
    }
   ],
   "source": [
    "trainer_m.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer_m.run() # multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 2.30539\n",
      "iter_dt 11.44ms; iter 100: train loss 1.93719\n",
      "iter_dt 11.02ms; iter 200: train loss 1.85809\n",
      "iter_dt 11.67ms; iter 300: train loss 1.72666\n",
      "iter_dt 12.58ms; iter 400: train loss 1.63121\n",
      "iter_dt 11.22ms; iter 500: train loss 1.57343\n",
      "iter_dt 13.34ms; iter 600: train loss 1.52472\n",
      "iter_dt 12.73ms; iter 700: train loss 1.50868\n",
      "iter_dt 12.93ms; iter 800: train loss 1.52555\n",
      "iter_dt 12.71ms; iter 900: train loss 1.45786\n",
      "iter_dt 12.66ms; iter 1000: train loss 1.45691\n",
      "iter_dt 12.57ms; iter 1100: train loss 1.38111\n",
      "iter_dt 10.63ms; iter 1200: train loss 1.44089\n",
      "iter_dt 12.60ms; iter 1300: train loss 1.38610\n",
      "iter_dt 12.43ms; iter 1400: train loss 1.42287\n",
      "iter_dt 12.73ms; iter 1500: train loss 1.44662\n",
      "iter_dt 10.76ms; iter 1600: train loss 1.39539\n",
      "iter_dt 10.56ms; iter 1700: train loss 1.38269\n",
      "iter_dt 11.49ms; iter 1800: train loss 1.36484\n",
      "iter_dt 10.72ms; iter 1900: train loss 1.37878\n",
      "iter_dt 12.62ms; iter 2000: train loss 1.37539\n",
      "iter_dt 10.48ms; iter 2100: train loss 1.40224\n",
      "iter_dt 11.30ms; iter 2200: train loss 1.36281\n",
      "iter_dt 11.76ms; iter 2300: train loss 1.33285\n",
      "iter_dt 10.70ms; iter 2400: train loss 1.28248\n",
      "iter_dt 12.69ms; iter 2500: train loss 1.29180\n",
      "iter_dt 11.76ms; iter 2600: train loss 1.30837\n",
      "iter_dt 11.08ms; iter 2700: train loss 1.38513\n",
      "iter_dt 12.76ms; iter 2800: train loss 1.33489\n",
      "iter_dt 12.69ms; iter 2900: train loss 1.30306\n",
      "iter_dt 12.73ms; iter 3000: train loss 1.32723\n",
      "iter_dt 13.08ms; iter 3100: train loss 1.34325\n",
      "iter_dt 13.55ms; iter 3200: train loss 1.38691\n",
      "iter_dt 11.82ms; iter 3300: train loss 1.32296\n",
      "iter_dt 12.73ms; iter 3400: train loss 1.30269\n",
      "iter_dt 12.59ms; iter 3500: train loss 1.32728\n",
      "iter_dt 12.60ms; iter 3600: train loss 1.40237\n",
      "iter_dt 12.55ms; iter 3700: train loss 1.38704\n",
      "iter_dt 12.64ms; iter 3800: train loss 1.36179\n",
      "iter_dt 11.79ms; iter 3900: train loss 1.30858\n",
      "iter_dt 12.65ms; iter 4000: train loss 1.29718\n",
      "iter_dt 12.74ms; iter 4100: train loss 1.33587\n",
      "iter_dt 11.73ms; iter 4200: train loss 1.25822\n",
      "iter_dt 11.25ms; iter 4300: train loss 1.28346\n",
      "iter_dt 14.45ms; iter 4400: train loss 1.32016\n",
      "iter_dt 12.65ms; iter 4500: train loss 1.27682\n",
      "iter_dt 11.57ms; iter 4600: train loss 1.30921\n",
      "iter_dt 11.83ms; iter 4700: train loss 1.27913\n",
      "iter_dt 14.73ms; iter 4800: train loss 1.27467\n",
      "iter_dt 12.73ms; iter 4900: train loss 1.30678\n",
      "iter_dt 10.67ms; iter 5000: train loss 1.29051\n",
      "iter_dt 12.58ms; iter 5100: train loss 1.25978\n",
      "iter_dt 12.69ms; iter 5200: train loss 1.26010\n",
      "iter_dt 13.72ms; iter 5300: train loss 1.28193\n",
      "iter_dt 12.45ms; iter 5400: train loss 1.29984\n",
      "iter_dt 12.40ms; iter 5500: train loss 1.26465\n",
      "iter_dt 11.35ms; iter 5600: train loss 1.24764\n",
      "iter_dt 12.70ms; iter 5700: train loss 1.25861\n",
      "iter_dt 12.44ms; iter 5800: train loss 1.24497\n",
      "iter_dt 12.75ms; iter 5900: train loss 1.27531\n",
      "iter_dt 11.41ms; iter 6000: train loss 1.24046\n",
      "iter_dt 10.37ms; iter 6100: train loss 1.28795\n",
      "iter_dt 10.87ms; iter 6200: train loss 1.25087\n",
      "iter_dt 12.19ms; iter 6300: train loss 1.29870\n",
      "iter_dt 11.49ms; iter 6400: train loss 1.27430\n",
      "iter_dt 12.68ms; iter 6500: train loss 1.28706\n",
      "iter_dt 12.20ms; iter 6600: train loss 1.29707\n",
      "iter_dt 13.07ms; iter 6700: train loss 1.25592\n",
      "iter_dt 11.02ms; iter 6800: train loss 1.24744\n",
      "iter_dt 10.59ms; iter 6900: train loss 1.23657\n",
      "iter_dt 11.06ms; iter 7000: train loss 1.21899\n",
      "iter_dt 12.65ms; iter 7100: train loss 1.23699\n",
      "iter_dt 12.79ms; iter 7200: train loss 1.22379\n",
      "iter_dt 12.71ms; iter 7300: train loss 1.26302\n",
      "iter_dt 11.10ms; iter 7400: train loss 1.20482\n",
      "iter_dt 12.62ms; iter 7500: train loss 1.21445\n",
      "iter_dt 12.79ms; iter 7600: train loss 1.15137\n",
      "iter_dt 12.75ms; iter 7700: train loss 1.09049\n",
      "iter_dt 11.57ms; iter 7800: train loss 1.14131\n",
      "iter_dt 10.66ms; iter 7900: train loss 1.11588\n",
      "iter_dt 12.76ms; iter 8000: train loss 1.12719\n",
      "iter_dt 13.15ms; iter 8100: train loss 1.08238\n",
      "iter_dt 12.68ms; iter 8200: train loss 1.03096\n",
      "iter_dt 10.46ms; iter 8300: train loss 1.03305\n",
      "iter_dt 12.74ms; iter 8400: train loss 0.99624\n",
      "iter_dt 10.45ms; iter 8500: train loss 1.01108\n",
      "iter_dt 12.74ms; iter 8600: train loss 0.99623\n",
      "iter_dt 12.60ms; iter 8700: train loss 1.02486\n",
      "iter_dt 11.37ms; iter 8800: train loss 1.03346\n",
      "iter_dt 11.04ms; iter 8900: train loss 0.99204\n",
      "iter_dt 12.35ms; iter 9000: train loss 1.03136\n",
      "iter_dt 12.66ms; iter 9100: train loss 1.00965\n",
      "iter_dt 11.61ms; iter 9200: train loss 0.98633\n",
      "iter_dt 12.66ms; iter 9300: train loss 1.06921\n",
      "iter_dt 13.14ms; iter 9400: train loss 0.97775\n",
      "iter_dt 11.87ms; iter 9500: train loss 1.01717\n",
      "iter_dt 12.69ms; iter 9600: train loss 0.96058\n",
      "iter_dt 10.62ms; iter 9700: train loss 1.00027\n",
      "iter_dt 12.74ms; iter 9800: train loss 0.99759\n",
      "iter_dt 10.48ms; iter 9900: train loss 1.04367\n"
     ]
    }
   ],
   "source": [
    "trainer_ml.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer_ml.run() # multiplication long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform some evaluation\n",
    "# model.eval()\n",
    "model_m.eval()\n",
    "# model_ml.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_add_split(trainer, split, max_batches, model, train_dataset, test_dataset):\n",
    "    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
    "    n = train_dataset.length # naugy direct access shrug\n",
    "    results = []\n",
    "    mistakes_printed_already = 0\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n",
    "    #loader = DataLoader(dataset, batch_size=1, num_workers=0, drop_last=False)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        y = y.to(trainer.device)\n",
    "\n",
    "        inp = x[:, :2*n]\n",
    "        sol = y[:, -n-1:]\n",
    "        \n",
    "        cat = model.generate(inp, n+1, do_sample=False) # using greedy argmax, not sampling\n",
    "        sol_candidate = cat[:, -n-1:]         \n",
    "        correct = (sol == sol_candidate).all(1).cpu() \n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "    \n",
    "    rt = torch.tensor(results, dtype=torch.float)\n",
    "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
    "    return rt.sum()\n",
    "\n",
    "# run a lot of examples from both train and test through the model and verify the output correctness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     train_score = eval_add_split(trainer, 'train', max_batches=50, model=model, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "#     test_score  = eval_add_split(trainer, 'test',  max_batches=50, model=model, train_dataset=train_dataset, test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     train_score \u001b[38;5;241m=\u001b[39m eval_add_split(trainer_m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_m, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset_m, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset_m)\n\u001b[1;32m      3\u001b[0m     test_score  \u001b[38;5;241m=\u001b[39m eval_add_split(trainer_m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,  max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_m, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset_m, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset_m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer_m' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_score = eval_add_split(trainer_m, 'train', max_batches=50, model=model_m, train_dataset=train_dataset_m, test_dataset=test_dataset_m)\n",
    "    test_score  = eval_add_split(trainer_m, 'test',  max_batches=50, model=model_m, train_dataset=train_dataset_m, test_dataset=test_dataset_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train final score: 1/10000 = 0.01% correct\n",
      "test final score: 3/10000 = 0.03% correct\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_score = eval_add_split(trainer_ml, 'train', max_batches=50, model=model_ml, train_dataset=train_dataset_ml, test_dataset=test_dataset_ml)\n",
    "    test_score  = eval_add_split(trainer_ml, 'test',  max_batches=50, model=model_ml, train_dataset=train_dataset_ml, test_dataset=test_dataset_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n",
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "model_config_m2 = GPT.get_default_config()\n",
    "# model_config_m2.model_type = 'gpt-mini'\n",
    "model_config_m2.model_type = 'gpt-micro'\n",
    "# model_config_m2.model_type = \"gpt2\"\n",
    "# model_config_m.model_type = 'gpt-nano'\n",
    "\n",
    "model_config_m2.vocab_size = train_dataset_m.get_vocab_size()\n",
    "model_config_m2.block_size = train_dataset_m.get_block_size()\n",
    "model_m2 = GPT(model_config_m2)\n",
    "\n",
    "\n",
    "train_config_m2 = Trainer.get_default_config()\n",
    "train_config_m2.num_workers = 0\n",
    "\n",
    "train_config_m2.learning_rate = 1e-3  # Lower learning rate for stability\n",
    "train_config_m2.max_iters = 10000     # Reduce if convergence is fast\n",
    "train_config_m2.batch_size = 4096       # Example batch size\n",
    "train_config_m2.weight_decay = 0.10\n",
    "\n",
    "trainer_m2 = Trainer(train_config_m2, model_m2, train_dataset_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 2.32143\n",
      "iter_dt 156.88ms; iter 100: train loss 1.62877\n",
      "iter_dt 144.56ms; iter 200: train loss 1.49800\n",
      "iter_dt 145.13ms; iter 300: train loss 1.41852\n",
      "iter_dt 145.67ms; iter 400: train loss 1.40050\n",
      "iter_dt 145.69ms; iter 500: train loss 1.34754\n",
      "iter_dt 145.95ms; iter 600: train loss 1.25344\n",
      "iter_dt 145.56ms; iter 700: train loss 1.19493\n",
      "iter_dt 145.99ms; iter 800: train loss 1.15087\n",
      "iter_dt 146.31ms; iter 900: train loss 1.12040\n",
      "iter_dt 151.63ms; iter 1000: train loss 1.07107\n",
      "iter_dt 233.32ms; iter 1100: train loss 1.03723\n",
      "iter_dt 146.37ms; iter 1200: train loss 1.01270\n",
      "iter_dt 235.12ms; iter 1300: train loss 0.96872\n",
      "iter_dt 146.32ms; iter 1400: train loss 0.92438\n",
      "iter_dt 145.90ms; iter 1500: train loss 0.90211\n",
      "iter_dt 146.36ms; iter 1600: train loss 0.87538\n",
      "iter_dt 147.02ms; iter 1700: train loss 0.84661\n",
      "iter_dt 146.31ms; iter 1800: train loss 0.82808\n",
      "iter_dt 146.03ms; iter 1900: train loss 0.81627\n",
      "iter_dt 146.17ms; iter 2000: train loss 0.80975\n",
      "iter_dt 146.59ms; iter 2100: train loss 0.78639\n",
      "iter_dt 146.93ms; iter 2200: train loss 0.77643\n",
      "iter_dt 147.26ms; iter 2300: train loss 0.77876\n",
      "iter_dt 146.30ms; iter 2400: train loss 0.76870\n",
      "iter_dt 146.24ms; iter 2500: train loss 0.74943\n",
      "iter_dt 235.97ms; iter 2600: train loss 0.73822\n",
      "iter_dt 146.90ms; iter 2700: train loss 0.75713\n",
      "iter_dt 146.52ms; iter 2800: train loss 0.72508\n",
      "iter_dt 146.92ms; iter 2900: train loss 0.70787\n",
      "iter_dt 145.70ms; iter 3000: train loss 0.70231\n",
      "iter_dt 146.12ms; iter 3100: train loss 0.70366\n",
      "iter_dt 146.34ms; iter 3200: train loss 0.69628\n",
      "iter_dt 146.24ms; iter 3300: train loss 0.65705\n",
      "iter_dt 233.82ms; iter 3400: train loss 0.64771\n",
      "iter_dt 146.49ms; iter 3500: train loss 0.67695\n",
      "iter_dt 146.04ms; iter 3600: train loss 0.64604\n",
      "iter_dt 146.26ms; iter 3700: train loss 0.61759\n",
      "iter_dt 145.51ms; iter 3800: train loss 0.61420\n",
      "iter_dt 145.59ms; iter 3900: train loss 0.59578\n",
      "iter_dt 145.40ms; iter 4000: train loss 0.57535\n",
      "iter_dt 146.36ms; iter 4100: train loss 0.57586\n",
      "iter_dt 145.58ms; iter 4200: train loss 0.57131\n",
      "iter_dt 146.14ms; iter 4300: train loss 0.55897\n",
      "iter_dt 145.76ms; iter 4400: train loss 0.55822\n",
      "iter_dt 146.23ms; iter 4500: train loss 0.53463\n",
      "iter_dt 145.74ms; iter 4600: train loss 0.54196\n",
      "iter_dt 146.41ms; iter 4700: train loss 0.54180\n",
      "iter_dt 145.85ms; iter 4800: train loss 0.52527\n",
      "iter_dt 232.27ms; iter 4900: train loss 0.53226\n",
      "iter_dt 146.08ms; iter 5000: train loss 0.51157\n",
      "iter_dt 145.77ms; iter 5100: train loss 0.49521\n",
      "iter_dt 145.60ms; iter 5200: train loss 0.49927\n",
      "iter_dt 146.25ms; iter 5300: train loss 0.49920\n",
      "iter_dt 146.08ms; iter 5400: train loss 0.48622\n",
      "iter_dt 146.93ms; iter 5500: train loss 0.46712\n",
      "iter_dt 146.47ms; iter 5600: train loss 0.47092\n",
      "iter_dt 231.26ms; iter 5700: train loss 0.46170\n",
      "iter_dt 146.32ms; iter 5800: train loss 0.44788\n",
      "iter_dt 146.68ms; iter 5900: train loss 0.44341\n",
      "iter_dt 145.50ms; iter 6000: train loss 0.42193\n",
      "iter_dt 147.00ms; iter 6100: train loss 0.40254\n",
      "iter_dt 230.33ms; iter 6200: train loss 0.40207\n",
      "iter_dt 146.03ms; iter 6300: train loss 0.37760\n",
      "iter_dt 145.75ms; iter 6400: train loss 0.36559\n",
      "iter_dt 146.31ms; iter 6500: train loss 0.39581\n",
      "iter_dt 146.13ms; iter 6600: train loss 0.34316\n",
      "iter_dt 146.70ms; iter 6700: train loss 0.34607\n",
      "iter_dt 145.47ms; iter 6800: train loss 0.34173\n",
      "iter_dt 146.01ms; iter 6900: train loss 0.32973\n",
      "iter_dt 145.90ms; iter 7000: train loss 0.31435\n",
      "iter_dt 146.08ms; iter 7100: train loss 0.31267\n",
      "iter_dt 146.09ms; iter 7200: train loss 0.30524\n",
      "iter_dt 145.99ms; iter 7300: train loss 0.30750\n",
      "iter_dt 145.96ms; iter 7400: train loss 0.29267\n",
      "iter_dt 232.32ms; iter 7500: train loss 0.28655\n",
      "iter_dt 145.70ms; iter 7600: train loss 0.28268\n",
      "iter_dt 146.37ms; iter 7700: train loss 0.27698\n",
      "iter_dt 146.50ms; iter 7800: train loss 0.28503\n",
      "iter_dt 145.80ms; iter 7900: train loss 0.27604\n",
      "iter_dt 146.06ms; iter 8000: train loss 0.26300\n",
      "iter_dt 146.57ms; iter 8100: train loss 0.26148\n",
      "iter_dt 145.62ms; iter 8200: train loss 0.26061\n",
      "iter_dt 145.92ms; iter 8300: train loss 0.26385\n",
      "iter_dt 146.61ms; iter 8400: train loss 0.24328\n",
      "iter_dt 145.53ms; iter 8500: train loss 0.25223\n",
      "iter_dt 146.19ms; iter 8600: train loss 0.24411\n",
      "iter_dt 146.14ms; iter 8700: train loss 0.25091\n",
      "iter_dt 231.76ms; iter 8800: train loss 0.24847\n",
      "iter_dt 146.63ms; iter 8900: train loss 0.24064\n",
      "iter_dt 234.06ms; iter 9000: train loss 0.24753\n",
      "iter_dt 146.03ms; iter 9100: train loss 0.23704\n",
      "iter_dt 146.76ms; iter 9200: train loss 0.22864\n",
      "iter_dt 147.04ms; iter 9300: train loss 0.22331\n",
      "iter_dt 149.70ms; iter 9400: train loss 0.22496\n",
      "iter_dt 165.84ms; iter 9500: train loss 0.22372\n",
      "iter_dt 157.12ms; iter 9600: train loss 0.21520\n",
      "iter_dt 145.68ms; iter 9700: train loss 0.20609\n",
      "iter_dt 146.90ms; iter 9800: train loss 0.21741\n",
      "iter_dt 146.40ms; iter 9900: train loss 0.19723\n"
     ]
    }
   ],
   "source": [
    "trainer_m2.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer_m2.run() # multiplication m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train final score: 2394/1000000 = 0.24% correct\n",
      "test final score: 2589/1000000 = 0.26% correct\n"
     ]
    }
   ],
   "source": [
    "model_m2.eval()\n",
    "with torch.no_grad():\n",
    "    train_score_m2 = eval_add_split(trainer_m2, 'train', max_batches=50, model=model_m2, train_dataset=train_dataset_m, test_dataset=test_dataset_m)\n",
    "    test_score_m2  = eval_add_split(trainer_m2, 'test',  max_batches=50, model=model_m2, train_dataset=train_dataset_m, test_dataset=test_dataset_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.001, batch_size=16, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 22/10000 = 0.22% correct\n",
      "Accuracy: 22.00%\n",
      "Testing parameters: lr=0.001, batch_size=16, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 13/10000 = 0.13% correct\n",
      "Accuracy: 13.00%\n",
      "Testing parameters: lr=0.001, batch_size=16, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 19/10000 = 0.19% correct\n",
      "Accuracy: 19.00%\n",
      "Testing parameters: lr=0.001, batch_size=32, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 20/10000 = 0.20% correct\n",
      "Accuracy: 20.00%\n",
      "Testing parameters: lr=0.001, batch_size=32, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 26/10000 = 0.26% correct\n",
      "Accuracy: 26.00%\n",
      "Testing parameters: lr=0.001, batch_size=32, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 17/10000 = 0.17% correct\n",
      "Accuracy: 17.00%\n",
      "Testing parameters: lr=0.001, batch_size=128, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 20/10000 = 0.20% correct\n",
      "Accuracy: 20.00%\n",
      "Testing parameters: lr=0.001, batch_size=128, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 19/10000 = 0.19% correct\n",
      "Accuracy: 19.00%\n",
      "Testing parameters: lr=0.001, batch_size=128, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 19/10000 = 0.19% correct\n",
      "Accuracy: 19.00%\n",
      "Testing parameters: lr=0.001, batch_size=256, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 25/10000 = 0.25% correct\n",
      "Accuracy: 25.00%\n",
      "Testing parameters: lr=0.001, batch_size=256, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 29/10000 = 0.29% correct\n",
      "Accuracy: 29.00%\n",
      "Testing parameters: lr=0.001, batch_size=256, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 21/10000 = 0.21% correct\n",
      "Accuracy: 21.00%\n",
      "Testing parameters: lr=0.001, batch_size=512, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 23/10000 = 0.23% correct\n",
      "Accuracy: 23.00%\n",
      "Testing parameters: lr=0.001, batch_size=512, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 23/10000 = 0.23% correct\n",
      "Accuracy: 23.00%\n",
      "Testing parameters: lr=0.001, batch_size=512, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 27/10000 = 0.27% correct\n",
      "Accuracy: 27.00%\n",
      "Testing parameters: lr=0.001, batch_size=1024, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 27/10000 = 0.27% correct\n",
      "Accuracy: 27.00%\n",
      "Testing parameters: lr=0.001, batch_size=1024, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 23/10000 = 0.23% correct\n",
      "Accuracy: 23.00%\n",
      "Testing parameters: lr=0.001, batch_size=1024, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 26/10000 = 0.26% correct\n",
      "Accuracy: 26.00%\n",
      "Testing parameters: lr=0.0045, batch_size=16, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 11/10000 = 0.11% correct\n",
      "Accuracy: 11.00%\n",
      "Testing parameters: lr=0.0045, batch_size=16, weight_decay=0.1\n",
      "running on device cuda\n",
      "test final score: 22/10000 = 0.22% correct\n",
      "Accuracy: 22.00%\n",
      "Testing parameters: lr=0.0045, batch_size=16, weight_decay=0.2\n",
      "running on device cuda\n",
      "test final score: 9/10000 = 0.09% correct\n",
      "Accuracy: 9.00%\n",
      "Testing parameters: lr=0.0045, batch_size=32, weight_decay=0.01\n",
      "running on device cuda\n",
      "test final score: 14/10000 = 0.14% correct\n",
      "Accuracy: 14.00%\n",
      "Testing parameters: lr=0.0045, batch_size=32, weight_decay=0.1\n",
      "running on device cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(train_config, model_m2, train_dataset_m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun()  \u001b[38;5;66;03m# You can also limit iterations here for faster search\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m eval_add_split(trainer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_m2, \n\u001b[1;32m     34\u001b[0m                           train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset_m, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset_m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/Ziob/wsniady/List_4/minGPT/mingpt/trainer.py:97\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Ziob/wsniady/miniconda3/envs/research2025/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/Ziob/wsniady/miniconda3/envs/research2025/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/Ziob/wsniady/miniconda3/envs/research2025/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 4.5e-3, 1e-2],\n",
    "    'batch_size': [16, 32, 128, 256, 512, 1024],\n",
    "    'weight_decay': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Get all combinations of parameters\n",
    "param_combinations = list(product(param_grid['learning_rate'], param_grid['batch_size'], param_grid['weight_decay']))\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "\n",
    "# Loop through all combinations of parameters\n",
    "for lr, batch_size, weight_decay in param_combinations:\n",
    "    print(f\"Testing parameters: lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}\")\n",
    "\n",
    "    # Configure model and trainer\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = lr\n",
    "    train_config.batch_size = batch_size\n",
    "    train_config.weight_decay = weight_decay\n",
    "    train_config.num_workers = 0  # Ensure compatibility with the system\n",
    "    train_config.max_iters = 10000\n",
    "    trainer = Trainer(train_config, model_m2, train_dataset_m)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.run()  # You can also limit iterations here for faster search\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = eval_add_split(trainer, 'test', max_batches=100, model=model_m2, \n",
    "                              train_dataset=train_dataset_m, test_dataset=test_dataset_m).mean()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Update best parameters if this is the highest accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {'learning_rate': lr, 'batch_size': batch_size, 'weight_decay': weight_decay}\n",
    "\n",
    "# Output the best parameters and accuracy\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research2025",
   "language": "python",
   "name": "research2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
